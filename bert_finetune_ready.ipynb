{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cc10895",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install --upgrade torch transformers datasets evaluate accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64c5fa1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\__init__.py:16: UserWarning: A NumPy version >=1.23.5 and <2.3.0 is required for this version of SciPy (detected version 2.3.0)\n",
      "  from scipy.sparse import issparse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(143613, 8) (15958, 8)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd, torch, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "DATA_PATH = \"data/processed/train.parquet\"  \n",
    "LABELS = [\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]\n",
    "df = pd.read_parquet(DATA_PATH)\n",
    "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42, stratify=df[\"toxic\"])\n",
    "print(train_df.shape, val_df.shape)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ff4c881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 143613\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 15958\n",
       " }))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def df_to_dataset(df):\n",
    "    encodings = tokenizer(\n",
    "        df[\"comment_text\"].tolist(),\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=128,\n",
    "    )\n",
    "    encodings[\"labels\"] = df[LABELS].astype(\"float32\").values.tolist()\n",
    "    return datasets.Dataset.from_dict(encodings)\n",
    "\n",
    "train_ds = df_to_dataset(train_df)\n",
    "val_ds   = df_to_dataset(val_df)\n",
    "train_ds, val_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72873210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Detected GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch, platform\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Detected GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f9ad730",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_5964\\4073449612.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='53856' max='53856' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [53856/53856 2:15:52, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.068600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.051500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.046600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.043500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.042900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.042800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.041300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.041200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.039500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.031800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.033700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.034200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.031900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.031600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.032400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.032100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.031400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.031700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.021000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.024200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.024400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.022500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.021600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.022600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.021900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.021700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=53856, training_loss=0.0337522425824379, metrics={'train_runtime': 8152.9077, 'train_samples_per_second': 52.845, 'train_steps_per_second': 6.606, 'total_flos': 2.834064379532851e+16, 'train_loss': 0.0337522425824379, 'epoch': 3.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=len(LABELS),\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = torch.sigmoid(torch.tensor(logits))\n",
    "    preds = (probs > 0.5).int().numpy()\n",
    "    macro_f1 = f1_score(labels, preds, average=\"macro\")\n",
    "    return {\"macro_f1\": macro_f1}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_runs\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    logging_steps=2000,      \n",
    "    save_steps=2000,         \n",
    "    do_eval=True,           \n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a8f3767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1995' max='1995' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1995/1995 00:59]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.044837553054094315, 'eval_macro_f1': 0.6749728084779072, 'eval_runtime': 59.7263, 'eval_samples_per_second': 267.186, 'eval_steps_per_second': 33.402, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "metrics = trainer.evaluate()\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae81c6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, thresh=0.5):\n",
    "\n",
    "    device = next(model.parameters()).device         \n",
    "\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=128,\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    probs = torch.sigmoid(logits)[0].cpu().numpy()    \n",
    "\n",
    "    prob_dict = {lab: float(p) for lab, p in zip(LABELS, probs)}\n",
    "    prob_dict[\"prediction\"] = [lab for lab, p in zip(LABELS, probs) if p > thresh]\n",
    "    return prob_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "36b3d3c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'toxic': 0.9844720959663391,\n",
       " 'severe_toxic': 0.00387908355332911,\n",
       " 'obscene': 0.04676353186368942,\n",
       " 'threat': 0.0010089100105687976,\n",
       " 'insult': 0.9472251534461975,\n",
       " 'identity_hate': 0.007398299407213926,\n",
       " 'prediction': ['toxic', 'insult']}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"You are the worst!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775ca683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD CHECKPOINT AND EVALUATE FROM HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbcf7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Saved predictions to: unseen_test_with_preds.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# --- Settings ---\n",
    "LABELS = [\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]\n",
    "CHECKPOINT = \"./bert_runs/checkpoint-53856\" \n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "TEST_CSV = \"test_samples.csv\"    \n",
    "OUT_CSV = \"unseen_test_with_preds.csv\"\n",
    "\n",
    "# --- Load Model & Tokenizer ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(CHECKPOINT)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# --- Prediction Helper ---\n",
    "def predict_batch(texts, thresh=0.5):\n",
    "    enc = tokenizer(\n",
    "        texts, \n",
    "        return_tensors=\"pt\", \n",
    "        truncation=True, \n",
    "        padding=True, \n",
    "        max_length=128\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**enc).logits\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "    preds = (probs > thresh).astype(int)\n",
    "    return probs, preds\n",
    "\n",
    "# --- Read Data ---\n",
    "df = pd.read_csv(TEST_CSV)\n",
    "\n",
    "# --- Predict in batches ---\n",
    "BATCH_SIZE = 32\n",
    "all_probs = []\n",
    "all_preds = []\n",
    "\n",
    "for i in range(0, len(df), BATCH_SIZE):\n",
    "    batch_texts = df[\"comment_text\"].iloc[i:i+BATCH_SIZE].tolist()\n",
    "    probs, preds = predict_batch(batch_texts)\n",
    "    all_probs.append(probs)\n",
    "    all_preds.append(preds)\n",
    "\n",
    "# Stack and add to df\n",
    "probs = np.vstack(all_probs)\n",
    "preds = np.vstack(all_preds)\n",
    "\n",
    "for i, label in enumerate(LABELS):\n",
    "    df[label] = probs[:, i]\n",
    "df[\"prediction\"] = [\n",
    "    [LABELS[j] for j, p in enumerate(row) if p == 1] \n",
    "    for row in preds\n",
    "]\n",
    "\n",
    "# --- Save ---\n",
    "df.to_csv(OUT_CSV, index=False)\n",
    "print(\"Done! Saved predictions to:\", OUT_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097bbb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 id                                       comment_text  toxic  \\\n",
      "0  d4ede52e6fc315a2  == Sky city == \\n\\n Hello, \\n\\n I noticed that...      0   \n",
      "1  345bb5c81a8a6797  \" \\n\\n \"\"Specifically, Allied forces suffered ...      0   \n",
      "2  73ef403fa5a095e6  \" \\n You know what, I think you're just insecu...     -1   \n",
      "3  d5290e73cf136173  \" \\n\\n == LOL == \\n\\n LOLLOLOLOLOOLOLLOLOLOLLO...     -1   \n",
      "4  d4911d1fd9989582  he is not a bad guy he is a sexy guy and his b...     -1   \n",
      "\n",
      "   severe_toxic  obscene  threat  insult  identity_hate  \n",
      "0             0        0       0       0              0  \n",
      "1             0        0       0       0              0  \n",
      "2            -1       -1      -1      -1             -1  \n",
      "3            -1       -1      -1      -1             -1  \n",
      "4            -1       -1      -1      -1             -1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "samples = pd.read_csv(\"test_samples.csv\")\n",
    "labels  = pd.read_csv(\"test_labels.csv\")\n",
    "\n",
    "merged = samples.merge(labels, on=\"id\", how=\"left\")\n",
    "\n",
    "print(merged.head())\n",
    "merged.to_csv(\"test_samples_with_labels.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17982a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Per-label confusion matrices:\n",
      "\n",
      "toxic\n",
      "         Pred 0  Pred 1\n",
      "True 0     338      25\n",
      "True 1       1      36\n",
      "\n",
      "severe_toxic\n",
      "         Pred 0  Pred 1\n",
      "True 0     393       3\n",
      "True 1       1       3\n",
      "\n",
      "obscene\n",
      "         Pred 0  Pred 1\n",
      "True 0     373       4\n",
      "True 1       6      17\n",
      "\n",
      "threat\n",
      "         Pred 0  Pred 1\n",
      "True 0     398       0\n",
      "True 1       1       1\n",
      "\n",
      "insult\n",
      "         Pred 0  Pred 1\n",
      "True 0     371       7\n",
      "True 1       6      16\n",
      "\n",
      "identity_hate\n",
      "         Pred 0  Pred 1\n",
      "True 0     395       1\n",
      "True 1       2       2\n",
      "\n",
      "Per-label Precision, Recall, F1, Accuracy, ROC-AUC:\n",
      "               Accuracy  Precision  Recall     F1  ROC-AUC\n",
      "Label                                                     \n",
      "toxic             0.935      0.590   0.973  0.735    0.984\n",
      "severe_toxic      0.990      0.500   0.750  0.600    0.997\n",
      "obscene           0.975      0.810   0.739  0.773    0.983\n",
      "threat            0.998      1.000   0.500  0.667    1.000\n",
      "insult            0.968      0.696   0.727  0.711    0.985\n",
      "identity_hate     0.992      0.667   0.500  0.571    0.997\n",
      "\n",
      "Macro/Micro Precision, Recall, F1, ROC-AUC:\n",
      "Macro: Precision=0.822, Recall=0.899, F1=0.856, ROC-AUC=0.990\n",
      "Micro: Precision=0.976, Recall=0.976, F1=0.976\n",
      "\n",
      "Scikit-learn per-label classification report:\n",
      "               precision  recall  f1-score  support\n",
      "toxic              0.590   0.973     0.735     37.0\n",
      "severe_toxic       0.500   0.750     0.600      4.0\n",
      "obscene            0.810   0.739     0.773     23.0\n",
      "threat             1.000   0.500     0.667      2.0\n",
      "insult             0.696   0.727     0.711     22.0\n",
      "identity_hate      0.667   0.500     0.571      4.0\n",
      "\n",
      "Mean samplewise accuracy (fraction of correct labels per row): 0.976\n",
      "Subset accuracy (exact match on all labels, only full rows): 0.890\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    precision_recall_fscore_support,\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "LABELS = [\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]\n",
    "\n",
    "preds = pd.read_csv(\"unseen_test_with_preds.csv\")\n",
    "truth = pd.read_csv(\"test_samples_with_labels.csv\")\n",
    "\n",
    "df = preds.merge(truth[[\"id\"] + LABELS], on=\"id\", suffixes=(\"_pred\", \"_true\"))\n",
    "\n",
    "for label in LABELS:\n",
    "    df[label + \"_pred_bin\"] = (df[label + \"_pred\"] > 0.5).astype(int)\n",
    "\n",
    "mask_matrix = []\n",
    "y_true_matrix = []\n",
    "y_pred_matrix = []\n",
    "y_prob_matrix = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    y_true_row = []\n",
    "    y_pred_row = []\n",
    "    y_prob_row = []\n",
    "    mask_row = []\n",
    "    for label in LABELS:\n",
    "        gt = row[label + \"_true\"]\n",
    "        if gt != -1:\n",
    "            mask_row.append(True)\n",
    "            y_true_row.append(gt)\n",
    "            y_pred_row.append(row[label + \"_pred_bin\"])\n",
    "            y_prob_row.append(row[label + \"_pred\"])\n",
    "        else:\n",
    "            mask_row.append(False)\n",
    "            y_true_row.append(0)  \n",
    "            y_pred_row.append(0)\n",
    "            y_prob_row.append(0)\n",
    "    y_true_matrix.append(y_true_row)\n",
    "    y_pred_matrix.append(y_pred_row)\n",
    "    y_prob_matrix.append(y_prob_row)\n",
    "    mask_matrix.append(mask_row)\n",
    "\n",
    "y_true_matrix = np.array(y_true_matrix)\n",
    "y_pred_matrix = np.array(y_pred_matrix)\n",
    "y_prob_matrix = np.array(y_prob_matrix)\n",
    "mask_matrix = np.array(mask_matrix, dtype=bool)\n",
    "\n",
    "def get_valid(label_idx):\n",
    "    valid = mask_matrix[:, label_idx]\n",
    "    return y_true_matrix[valid, label_idx], y_pred_matrix[valid, label_idx], y_prob_matrix[valid, label_idx]\n",
    "\n",
    "print(\"=\"*40)\n",
    "print(\"Per-label confusion matrices:\")\n",
    "for i, label in enumerate(LABELS):\n",
    "    y_true_lbl, y_pred_lbl, _ = get_valid(i)\n",
    "    cm = confusion_matrix(y_true_lbl, y_pred_lbl)\n",
    "    print(f\"\\n{label}\\n\", pd.DataFrame(cm, index=[\"True 0\", \"True 1\"], columns=[\"Pred 0\", \"Pred 1\"]))\n",
    "\n",
    "print(\"\\nPer-label Precision, Recall, F1, Accuracy, ROC-AUC:\")\n",
    "stats = []\n",
    "for i, label in enumerate(LABELS):\n",
    "    y_true_lbl, y_pred_lbl, y_prob_lbl = get_valid(i)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        y_true_lbl, y_pred_lbl, average='binary', zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(y_true_lbl, y_pred_lbl)\n",
    "    try:\n",
    "        auc = roc_auc_score(y_true_lbl, y_prob_lbl)\n",
    "    except Exception:\n",
    "        auc = np.nan\n",
    "    stats.append([label, acc, prec, rec, f1, auc])\n",
    "df_stats = pd.DataFrame(stats, columns=[\"Label\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"ROC-AUC\"]).set_index(\"Label\")\n",
    "print(df_stats.round(3))\n",
    "\n",
    "print(\"\\nMacro/Micro Precision, Recall, F1, ROC-AUC:\")\n",
    "flat_true, flat_pred, flat_prob = [], [], []\n",
    "for i in range(len(LABELS)):\n",
    "    y_true_lbl, y_pred_lbl, y_prob_lbl = get_valid(i)\n",
    "    flat_true += list(y_true_lbl)\n",
    "    flat_pred += list(y_pred_lbl)\n",
    "    flat_prob += list(y_prob_lbl)\n",
    "\n",
    "prec_macro, rec_macro, f1_macro, _ = precision_recall_fscore_support(flat_true, flat_pred, average=\"macro\", zero_division=0)\n",
    "prec_micro, rec_micro, f1_micro, _ = precision_recall_fscore_support(flat_true, flat_pred, average=\"micro\", zero_division=0)\n",
    "try:\n",
    "    roc_auc_macro = roc_auc_score(flat_true, flat_prob, average=\"macro\")\n",
    "except Exception:\n",
    "    roc_auc_macro = np.nan\n",
    "\n",
    "print(f\"Macro: Precision={prec_macro:.3f}, Recall={rec_macro:.3f}, F1={f1_macro:.3f}, ROC-AUC={roc_auc_macro:.3f}\")\n",
    "print(f\"Micro: Precision={prec_micro:.3f}, Recall={rec_micro:.3f}, F1={f1_micro:.3f}\")\n",
    "\n",
    "masked_y_true = np.where(mask_matrix, y_true_matrix, np.nan)\n",
    "masked_y_pred = np.where(mask_matrix, y_pred_matrix, np.nan)\n",
    "\n",
    "report_dict = {}\n",
    "for i, label in enumerate(LABELS):\n",
    "    y_true_lbl, y_pred_lbl, _ = get_valid(i)\n",
    "    report = classification_report(y_true_lbl, y_pred_lbl, output_dict=True, zero_division=0)\n",
    "    report_dict[label] = report['1']\n",
    "\n",
    "df_report = pd.DataFrame(report_dict).T[['precision', 'recall', 'f1-score', 'support']]\n",
    "print(\"\\nScikit-learn per-label classification report:\")\n",
    "print(df_report.round(3))\n",
    "\n",
    "sample_acc = []\n",
    "for row_true, row_pred, row_mask in zip(y_true_matrix, y_pred_matrix, mask_matrix):\n",
    "    n_valid = row_mask.sum()\n",
    "    if n_valid == 0:\n",
    "        continue\n",
    "    sample_acc.append((row_true[row_mask] == row_pred[row_mask]).mean())\n",
    "print(f\"\\nMean samplewise accuracy (fraction of correct labels per row): {np.mean(sample_acc):.3f}\")\n",
    "\n",
    "all_valid_mask = mask_matrix.all(axis=1)\n",
    "if all_valid_mask.sum():\n",
    "    subset_acc = np.mean([np.array_equal(y_true_matrix[i], y_pred_matrix[i]) for i in range(len(y_true_matrix)) if all_valid_mask[i]])\n",
    "    print(f\"Subset accuracy (exact match on all labels, only full rows): {subset_acc:.3f}\")\n",
    "\n",
    "print(\"=\"*40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e648df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
