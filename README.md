

---

## üß¨ Dataset

- **Source**: [Jigsaw Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge)
- **Training Data**:
    - `data/processed/train.parquet`: Used for the TF-IDF baseline and BERT fine-tuning.
    - `train.csv` (original, expected in `/content/` for the Colab notebook, or `data/raw/`): Used for Gemma-3 fine-tuning. This dataset should contain `comment_text` and the six binary label columns.
- **Test Data**:
    - `data/raw/test.csv` (or `test_samples.csv` if pre-processed): Contains comment IDs and text.
    - `data/raw/test_labels.csv`: Contains comment IDs and corresponding true labels (-1 for missing labels).
    - `data/processed/test_samples_with_labels.csv`: Generated by merging `test.csv` and `test_labels.csv`; used for evaluation in several notebooks.
- **Size**: Approx. 159,571 rows for training.
- **Labels**: `toxic`, `severe_toxic`, `obscene`, `threat`, `insult`, `identity_hate`
- **Type**: Multi-label classification.
- **Note**: Significant class imbalance, especially in `threat` and `severe_toxic`.

---

## ‚öôÔ∏è Setup and Installation

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/YOUR_USERNAME/toxicity-classification.git # Replace YOUR_USERNAME
    cd toxicity-classification
    ```

2.  **Create a virtual environment (recommended):**
    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows: venv\Scripts\activate
    ```

3.  **Install dependencies:**
    Based on the notebooks, key dependencies include:
    ```bash
    pip install pandas scikit-learn iterstrat-ml_stratifiers joblib \
                torch torchvision torchaudio transformers[sentencepiece] datasets evaluate accelerate \
                openai aiofiles nest_asyncio matplotlib seaborn \
                unsloth trl bitsandbytes peft protobuf \
                huggingface_hub hf_transfer google-generativeai
    ```
    *   For `unsloth` on systems other than Colab, you might use `pip install "unsloth[colab-newest-bitsandbytes@latest]" --upgrade` or refer to official Unsloth installation guides.
    *   Ensure you have a compatible PyTorch version with CUDA support if you plan to use GPUs for BERT and Gemma fine-tuning.

4.  **API Keys & Tokens:**
    *   **Gemini API**: For `Gemini_2_5_Pro_Cluster.ipynb`, you will need a Google AI Studio API key. Set it as an environment variable `GOOGLE_API_KEY` or directly in the notebook.
    *   **Hugging Face Hub**: For `Gemma3_(4B)_finetune.ipynb` (pushing to Hub), a Hugging Face Hub token is required. Login via `huggingface-cli login`.

5.  **Data:**
    *   Place your training data (`train.parquet` or `train.csv`) and test data (`test.csv`, `test_labels.csv`) in the paths expected by the notebooks (e.g., `data/raw/`, `data/processed/`, or `/content/` for Colab).
    *   Some notebooks might generate `test_samples_with_labels.csv` by merging test data and labels.

---

## üöÄ Running the Models & Experiments

This repository contains Jupyter notebooks, each implementing a different approach:

### 1. Baseline TF-IDF + Logistic Regression
*   **Notebook**: `notebooks/baseline_tfidf_ready.ipynb`
*   **Process**: Implements a TF-IDF vectorizer and a OneVsRest Logistic Regression classifier. Trains the model on `data/processed/train.parquet`, saves it to `models/tfidf_logreg.pkl`, and evaluates on `data/processed/test_samples_with_labels.csv`.
*   **Output**: `models/tfidf_logreg.pkl`, classification metrics, predictions saved to CSV (e.g., `results/tfidf_logreg_predictions.csv`).

### 2. BERT Fine-Tuning
*   **Notebook**: `notebooks/bert_finetune_ready.ipynb`
*   **Process**: Fine-tunes a `bert-base-uncased` model for multi-label toxic comment classification using Hugging Face `transformers`, `datasets`, and `Trainer` API on `data/processed/train.parquet`.
*   **Output**: Fine-tuned model checkpoint (e.g., `models/bert_finetuned_model/`), evaluation metrics, predictions saved to CSV (e.g., `results/bert_predictions.csv`).

### 3. Gemini 2.5 Pro (Prompt-based)
*   **Notebook**: `notebooks/Gemini_2_5_Pro_Cluster.ipynb`
*   **Process**: Leverages the Gemini 2.5 Pro model via the Google Generative AI API. Uses a detailed system prompt and function calling to classify comments from `test_samples.csv` into a *single most severe* toxicity label or "none". Processes comments asynchronously.
*   **API Key**: Requires a Google API Key (e.g., from Google AI Studio).
*   **Output**: `results/gemini_test_results.csv`, evaluation using "loose match accuracy" and binary confusion matrix.

### 4. Gemma 3 4B (Fine-tuned with LoRA)
*   **Finetuning Notebook**: `notebooks/Gemma3_(4B)_finetune.ipynb`
*   **Process**: Fine-tunes the `unsloth/gemma-3-4b-it` model using Unsloth and LoRA for efficient training on `train.csv`. The model is trained to output comma-separated toxicity labels.
*   **Model**: Adapters are saved and pushed to Hugging Face Hub: `Subhrato20/gemma-3-toxic-v2`.
*   **Inference/Evaluation Notebook**: `notebooks/Gemma3_classification.ipynb` (Assumed)
    *   Likely loads the fine-tuned Gemma model (base + LoRA adapters) for inference on test data.
    *   Evaluates multi-label classification performance.
*   **Output**: `results/gemma_test_results.csv` (from inference notebook).

---

## üß™ Evaluation Metrics

*   **Per-label (for multi-label models like TF-IDF, BERT, Gemma)**: Accuracy, Precision, Recall, F1-score, ROC-AUC.
*   **Aggregate (for multi-label models)**: Macro F1, Micro F1, Sample-wise Accuracy, Subset Accuracy.
*   **Confusion Matrices**: Visualized per model (multi-label or binary as appropriate).
*   **Loose Match Accuracy**: Used for Gemini (single most severe label prediction) & potentially for Gemma if evaluating based on primary predicted labels. This metric assesses if at least one of the predicted labels matches one of the true labels, or if both predict "none".

---

## üìà Key Results

| Model                       | Macro-F1 | Subset Accuracy | Loose Accuracy | Notes                                                                 |
| --------------------------- | -------- | --------------- | -------------- | --------------------------------------------------------------------- |
| TF-IDF + LogReg             | 0.787    | 0.838           | 0.838 (as subset) | Fast, interpretable baseline. Evaluated multi-label.                  |
| BERT Fine-Tuned             | **0.856**| **0.890**       | 0.890 (as subset) | Best overall multi-label performance.                                   |
| Gemini 2.5 Pro (Prompted)   | N/A      | N/A             | 0.7475         | Zero-shot via prompt engineering, predicts *single most severe* label.  |
| Gemma 3 4B (LoRA Fine-tuned)| N/A    | N/A          | 0.8275         | Finetuned SLM, efficient & strong. Multi-label output, Loose Accuracy reported. |

*Note on Comparability:*
*   TF-IDF and BERT are evaluated on full multi-label classification. Their "Loose Accuracy" here is considered equivalent to "Subset Accuracy" for direct comparison purposes if all labels must match.
*   Gemini 2.5 Pro is prompted for a single most severe label.
*   Gemma-3 4B is fine-tuned for multi-label output (comma-separated). Its "Loose Accuracy" likely reflects a relaxed matching criterion. Detailed multi-label metrics (Macro-F1, Subset Accuracy) would come from `Gemma3_classification.ipynb`.

---

## ‚ö†Ô∏è Challenges & Limitations

1.  **Class Imbalance**:
    *   `threat` and `severe_toxic` are under-represented, leading to low recall on these rare classes.
    *   Potential Mitigation: Focal loss, oversampling techniques (e.g., SMOTE), class weighting.

2.  **Bias in Training Data**:
    *   Models might misclassify reclaimed slurs or comments with complex sentiment.
    *   Potential Mitigation: Debiasing techniques, careful data augmentation, incorporating context rules.

3.  **Resource Requirements**:
    *   BERT fine-tuning took ‚âà22 min on an RTX 3060. Gemma-3 fine-tuning requires a capable GPU and benefits from 4-bit quantization (via Unsloth). LLM API calls can incur costs.
    *   Potential Mitigation: Using smaller pre-trained models (e.g., DistilBERT), model distillation, quantization.

4.  **Contextual Nuance & Ambiguity**:
    *   Sarcasm, cultural references, and evolving language can lead to false positives/negatives.
    *   LLMs with prompting (like Gemini) might struggle with strict adherence to multiple fine-grained labels compared to fine-tuned models.
    *   Future: Incorporate pragmatic/contextual embeddings, multilingual training, more sophisticated prompt engineering.

---

## üöÄ Future Work

*   **Data Balancing**: Implement focal loss or synthetic oversampling (e.g., SMOTE) for rare classes.
*   **Fairness and Bias**: Explore techniques like adversarial training or context-aware rules to mitigate bias related to identity terms.
*   **Model Efficiency**: Experiment with `DistilBERT`, `RoBERTa-small`, or further model distillation techniques.
*   **Multilingual Toxicity**: Extend the models and dataset to handle toxicity detection in multiple languages.
*   **Advanced Prompting for LLMs**: For models like Gemini, explore few-shot prompting, chain-of-thought, or self-consistency to improve multi-label discernment.
*   **Ensemble Methods**: Combine predictions from different models to potentially improve robustness and accuracy.

---

## üìú License
This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.

---
## ‚úçÔ∏è Author
*   **Subhrato Som** (and team members as listed above)

---
---
## üôè Acknowledgements
*   This project utilizes the Jigsaw/Conversation AI dataset from their Toxic Comment Classification Challenge on Kaggle.
*   Built with invaluable open-source libraries including Scikit-learn, PyTorch, Hugging Face (Transformers, Datasets, Evaluate, Hub), Unsloth, Pandas, and others.
*   The Gemma-3 fine-tuning notebook significantly benefits from the Unsloth library for efficient model training.
*   The exploration of efficient model adaptation techniques in this project, such as LoRA for fine-tuning Gemma, draws inspiration from the rapidly advancing research landscape. We acknowledge influential works in this area, including novel approaches like those described in "[Text-to-LoRA: Instant Transformer Adaption](https://arxiv.org/abs/2506.06105)" by R. Charakorn, E. Cetin, Y. Tang, and R. T. Lange (arXiv:2506.06105; ICML 2025), which proposes methods for generating LoRA adapters directly from natural language task descriptions.
